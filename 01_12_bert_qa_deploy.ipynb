{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNs2RfljGU+okiKX9W5N4v4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zzwony/Start_0920/blob/main/01_12_bert_qa_deploy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIHnkhMydN38"
      },
      "outputs": [],
      "source": [
        "!pip install ratsnlp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount=True)"
      ],
      "metadata": {
        "id": "KCWHMGG1dsto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 인퍼천스 설정\n",
        "\n",
        "from ratsnlp.nlpbook.qa import QADeployArguments\n",
        "args = QADeployArguments(\n",
        "    pretrained_model_name=\"beomi/kcbert-base\",\n",
        "    downstream_model_dir=\"/gdrive/My Drive/nlpbook/checkpoint-qa\",\n",
        "    max_seq_length=128,\n",
        "    max_query_length=32,\n",
        ")"
      ],
      "metadata": {
        "id": "VTXmx--0dswA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 토크나이저 로드\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\n",
        "    args.pretrained_model_name,\n",
        "    do_lower_case=False,\n",
        ")\n",
        "\n",
        "# 체크포인트 로드\n",
        "\n",
        "import torch\n",
        "fine_tuned_model_ckpt = torch.load(\n",
        "    args.downstream_model_checkpoint_path,\n",
        "    map_location=torch.device(\"cpu\")\n",
        ")"
      ],
      "metadata": {
        "id": "gxYgDugHdsyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BERT 설정 로드\n",
        "\n",
        "from transformers import BertConfig\n",
        "pretrained_model_config = BertConfig.from_pretrained(\n",
        "    args.pretrained_model_name,\n",
        ")\n",
        "\n",
        "# 모델 초기화\n",
        "\n",
        "from transformers import BertForQuestionAnswering\n",
        "model = BertForQuestionAnswering(pretrained_model_config)"
      ],
      "metadata": {
        "id": "NuaQwL-0ds0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 체크포인트 읽기\n",
        "model.load_state_dict({k.replace(\"model.\", \"\"): v for k, v in fine_tuned_model_ckpt['state_dict'].items()})\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "chGOg9NshL4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 인퍼런스\n",
        "\n",
        "def inference_fn(question, context):\n",
        "    if question and context:\n",
        "        truncated_query = tokenizer.encode(\n",
        "            question,\n",
        "            add_special_tokens=False,\n",
        "            truncation=True,\n",
        "            max_length=args.max_query_length\n",
        "       )\n",
        "        inputs = tokenizer.encode_plus(\n",
        "            text=truncated_query,\n",
        "            text_pair=context,\n",
        "            truncation=\"only_second\",## pair 문장이 주어졌을 경우 두번재 문자엥 대해서만 truncation\n",
        "            padding=\"max_length\",\n",
        "            max_length=args.max_seq_length,\n",
        "            return_token_type_ids=True,\n",
        "        )\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**{k: torch.tensor([v]) for k, v in inputs.items()})\n",
        "            start_pred = outputs.start_logits.argmax(dim=-1).item()## 시작 토큰에 해당하는 로짓(확률값들)에서 가장 큰 인덱스\n",
        "            end_pred = outputs.end_logits.argmax(dim=-1).item()## 마지막 토큰에 해당하는 로짓(확률값들)에서 가장 큰 인덱스\n",
        "            pred_text = tokenizer.decode(inputs['input_ids'][start_pred:end_pred+1])\n",
        "    else:\n",
        "        pred_text = \"\"\n",
        "    return {\n",
        "        'question': question,\n",
        "        'context': context,\n",
        "        'answer': pred_text,\n",
        "    }"
      ],
      "metadata": {
        "id": "QCUHHPIChL63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 웹 서비스\n",
        "\n",
        "from ratsnlp.nlpbook.qa import get_web_service_app\n",
        "app = get_web_service_app(inference_fn)\n",
        "app.run()"
      ],
      "metadata": {
        "id": "N3dlVq8ohL9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QU6IvMrbhL_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IZ2334oDhMBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WF-W15tRhMC_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}