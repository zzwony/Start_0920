{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1DbLm-ARm1OwjpfGNRjQ8HSn0Czr5w178",
      "authorship_tag": "ABX9TyMLhkr4iUhPqstq53GCR+H1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zzwony/Start_0920/blob/main/12_09.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hwec762Brgbk"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()  ## 이건 load_data에 있는걸 그냥 가져온것이다.\n",
        "x_train, x_test = x_train/255.0, x_test/255.0  ## 실수로 만들어줘야 해서 .0을 해줘야한다. ## 0과 1사이로 스케일 조정"
      ],
      "metadata": {
        "id": "l4XajgWVs802"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train[0]\n",
        "print(x_train.shape, x_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUzKHKbNtVXJ",
        "outputId": "dacc82a9-369a-4fd1-dba1-9bf2145bd34c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28) (10000, 28, 28)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import Sequential, models, layers\n",
        "\n",
        "network = models.Sequential()\n",
        "network.add(layers.Dense(512, activation='relu', input_shape=(28*28,))) ## 512는 아웃풋이다.\n",
        "## relu는 비선형 변환을 해준다. 왜 비선형 변환을 해줄까?\n",
        "## 선형 변환만 계속 하게 되면 네트워크를 많이 쌓게 되는데 그건 선형변환 1개 있는것과 같다.\n",
        "network.add(layers.Dense(10, activation='softmax'))  ## 최종 아웃풋 10개(mnist는 0에서 9까지 총 10개를 분류하기 때문)\n",
        "\n",
        "network.compile(optimizer='rmsprop', \n",
        "                loss = 'sparse_categorical_crossentropy', \n",
        "                metrics = 'acc')\n",
        "## optimizer는 훈련을 어떻게 최적화 할것인가?, 멀티 분류할때는 sparse_categorical_crossentropy을 사용한다."
      ],
      "metadata": {
        "id": "rYbdVYKbvuZI"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5fEO_NyF24-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LyF_P7wa25FS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L8mLGIHO25Lq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
      ],
      "metadata": {
        "id": "PtcsruaZ2trc"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_images = train_images.reshape((60000, 28*28))\n",
        "train_images = train_images.astype('float32') / 255\n",
        "test_images = test_images.reshape((10000, 28*28))\n",
        "test_images = test_images.astype('float32') / 255"
      ],
      "metadata": {
        "id": "KxooePnpyQiD"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_images[0]))\n",
        "train_images.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BttMsD9a2HlY",
        "outputId": "1af2c6a4-0a71-4288-a65f-2cad6a84120c"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "784\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 784)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcpgJMg73QR1",
        "outputId": "2849a9a8-c7dc-4059-bc7a-698cd54d05ab"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000,)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "network.fit(train_images, train_labels, epochs=5, batch_size=128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IT0YJ2903ECO",
        "outputId": "58dad496-99e3-4b4f-c60a-68c914779a35"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.2566 - acc: 0.9259\n",
            "Epoch 2/5\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.1038 - acc: 0.9697\n",
            "Epoch 3/5\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0683 - acc: 0.9797\n",
            "Epoch 4/5\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0503 - acc: 0.9848\n",
            "Epoch 5/5\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0375 - acc: 0.9890\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7efc044ec730>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 평가\n",
        "test_loss, test_acc = network.evaluate(test_images, test_labels)\n",
        "print('test_acc:', test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2au5FOxZ3vGl",
        "outputId": "606bf1dd-f5ba-4079-9ee8-6915d1eaa119"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0635 - acc: 0.9803\n",
            "test_acc: 0.9803000092506409\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W4V7IgkP_RaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EA1043zM_Sya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SQXKj5sG_S_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32,(3,3), activation='relu', input_shape=(28,28,1)))\n",
        "## 앞에 32 랑 3,3은 이미지 갯수랑 필터 크기이다.\n",
        "model.add(layers.MaxPooling2D((2,2)))  ## 풀링은 세로 가로 방향의 공간을 줄이는 연산\n",
        "model.add(layers.Conv2D(64,(3,3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2,2)))\n",
        "model.add(layers.Conv2D(64,(3,3), activation='relu'))\n"
      ],
      "metadata": {
        "id": "vh-I6wnM_RhS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHkLHrKRTYJu",
        "outputId": "db04e089-427a-4085-db1d-785ca8742525"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 13, 13, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 5, 5, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 3, 3, 64)          36928     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 55,744\n",
            "Trainable params: 55,744\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## conv2d의 shape를 보면 패딩 때문에 2개씩 줄어들었다.\n",
        "## 여기서 끝? 아니다. 완전연결층에 연결시켜줘야한다.(플레튼 과정을 거친다.)"
      ],
      "metadata": {
        "id": "qN8OjQIxUrf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.add(layers.Flatten()) ## Flatten이라는건 평평하게 핀다는 뜻\n",
        "model.add(layers.Dense(64, activation='relu')) # 완전 연결층은 Dense이다.\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_7hh4vPVbdb",
        "outputId": "4ef92909-b792-4cc5-ef13-defe065a7595"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 13, 13, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 5, 5, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 3, 3, 64)          36928     \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 576)               0         \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 576)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                36928     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                650       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 93,322\n",
            "Trainable params: 93,322\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##공간 정보를 잘 학습해서 잘 담은 상태에서 완전 연결층으로 처리해주는게 Flatten 역할\n",
        "##(학습한 형태만 바꿔주는것이다.)\n",
        "\n",
        "##처음부터 완전 연결층을 해주면 제대로 공간 학습을 할 수 없어서 이 과정이 필요하다."
      ],
      "metadata": {
        "id": "TLBdFFhuVyLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "train_images = train_images.reshape((60000, 28, 28, 1))\n",
        "train_images = train_images.astype('float32') / 255\n",
        "test_images = test_images.reshape((10000, 28, 28, 1))\n",
        "test_images = test_images.astype('float32') / 255\n",
        "\n",
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YyLXss0CWGh0",
        "outputId": "af2e44ac-82e5-46ec-9629-383a1ba5ab25"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(train_images, train_labels, epochs=5, batch_size=64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlXn_a2tXBM6",
        "outputId": "9641dd90-e6d6-4fe4-bc5e-cb6b0abbd98b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "938/938 [==============================] - 80s 84ms/step - loss: 0.1661 - accuracy: 0.9490\n",
            "Epoch 2/5\n",
            "938/938 [==============================] - 62s 66ms/step - loss: 0.0466 - accuracy: 0.9853\n",
            "Epoch 3/5\n",
            "938/938 [==============================] - 65s 69ms/step - loss: 0.0334 - accuracy: 0.9896\n",
            "Epoch 4/5\n",
            "938/938 [==============================] - 59s 63ms/step - loss: 0.0246 - accuracy: 0.9924\n",
            "Epoch 5/5\n",
            "938/938 [==============================] - 71s 76ms/step - loss: 0.0199 - accuracy: 0.9939\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc12f3d3730>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print('정확도:', test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyCJt9mTXdxH",
        "outputId": "7a768bc1-572c-41a4-e687-710579189d6b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 4s 12ms/step - loss: 0.0347 - accuracy: 0.9889\n",
            "정확도: 0.9889000058174133\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 에폭은 왜 해? 가중치를 최대한 계산하기 위함\n",
        "## 그럼 많이 하면 좋아? 아니다. 비용과 시간이 많이 걸린다는 단점이 있다.\n",
        "\n",
        "## 우리의 목표는 모델의 일반화, 그래서 우리가 과대적합을 보는것이다."
      ],
      "metadata": {
        "id": "YFf6bTZhXuR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "845psIc5ZKG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ocppxq-QZXw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras==2.3.1\n",
        "!pip install tensorflow==2.2.0"
      ],
      "metadata": {
        "id": "u_XzV3bMZX1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# shutil 패키지를 사용하여 구조 만들기\n",
        "#이미지를 훈련, 검증, 테스트 디렉터리로 복사하기\n",
        "import os, shutil, pathlib\n",
        "\n",
        "original_dir = pathlib.Path('train')\n",
        "## 원본 데이터셋이 압축 해제되어 있는 디렉터리 경로\n",
        "new_base_dir = pathlib.Path('cats_vs_dogs_small')\n",
        "## 서브셋 데이터를 저장할 디렉터리\n",
        "\n",
        "def make_subset(subset_name, start_index, end_index):\n",
        "    for category in ('cat', 'dog'):\n",
        "        dir = new_base_dir / subset_name / category\n",
        "        os.makedirs(dir)\n",
        "        fnames = [f'{category}.{i}.jpg'\n",
        "                 for i in range(start_index, end_index)]\n",
        "        for fname in fnames:\n",
        "            shutil.copyfile(src=original_dir / fname,\n",
        "                           dst=dir / fname)\n",
        "            \n",
        "make_subset('train', start_index=0, end_index=1000)  ## 카테고리마다 처음 1000개의 이미지를 훈련 서브셋으로 만들기.\n",
        "make_subset('validation', start_index=1000, end_index=1500)  ## 카테고리마다 그다음 500개의 이미지를 검증 서브셋으로 만들기.\n",
        "make_subset('test', start_index=1500, end_index=2000)  ## 카테고리마다 그다음 1000개의 이미지를 테스트 서브셋으로 만들기."
      ],
      "metadata": {
        "id": "I0U6-WZrmNut"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "model = models.Sequential()\n",
        "\n",
        "model.add(layers.Conv2D(32,(3,3), activation='relu', input_shape=(180, 180, 3)))\n",
        "model.add(layers.MaxPooling2D((2,2)))\n",
        "model.add(layers.Conv2D(64,(3,3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2,2)))\n",
        "model.add(layers.Conv2D(128,(3,3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2,2)))\n",
        "model.add(layers.Conv2D(256,(3,3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2,2)))\n",
        "model.add(layers.Conv2D(256,(3,3), activation='relu'))\n",
        "\n",
        "model.add(layers.Flatten()) \n",
        "model.add(layers.Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "O6gClQ1Ig-zL"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4eNEfrCh6iS",
        "outputId": "8a6a9440-6828-4b01-c7ee-7af907fddb2a"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_16 (Conv2D)           (None, 178, 178, 32)      896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_12 (MaxPooling (None, 89, 89, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 87, 87, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_13 (MaxPooling (None, 43, 43, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 41, 41, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_14 (MaxPooling (None, 20, 20, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 18, 18, 256)       295168    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_15 (MaxPooling (None, 9, 9, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 7, 7, 256)         590080    \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 12544)             0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 12545     \n",
            "=================================================================\n",
            "Total params: 991,041\n",
            "Trainable params: 991,041\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy',\n",
        "             optimizer='rmsprop',\n",
        "             metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "8f2aS3sQiDTy"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "id": "taacMJGRlJKm",
        "outputId": "0d9569c8-2715-45a2-df1d-e7d97bd5e09a"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2a88e565-dfe0-4d9d-8367-e2a50f0a83f9\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2a88e565-dfe0-4d9d-8367-e2a50f0a83f9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-5c2e8a8d365b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m   \"\"\"\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m   \u001b[0;31m# First result is always an indication that the file picker has completed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m   result = _output.eval_js(\n\u001b[0m\u001b[1;32m    145\u001b[0m       'google.colab._files._uploadFiles(\"{input_id}\", \"{output_id}\")'.format(\n\u001b[1;32m    146\u001b[0m           input_id=input_id, output_id=output_id))\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "m-Uiea8ilSbF"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c dogs-vs-cats\n",
        "!unzip -qq dogs-vs-cats.zip\n",
        "!unzip -qq train.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Ikrgv7Nj_WW",
        "outputId": "752f6555-719b-46d9-c2e8-9a6571d750bf"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading dogs-vs-cats.zip to /content\n",
            " 98% 797M/812M [00:04<00:00, 211MB/s]\n",
            "100% 812M/812M [00:04<00:00, 181MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import image_dataset_from_directory\n",
        "train_dataset = image_dataset_from_directory(  ## 겉보기엔 데이터셋으로 보이지만 제너레이터라는 기계이다.\n",
        "    new_base_dir / 'train',\n",
        "    image_size=(180, 180),\n",
        "    batch_size=32)\n",
        "\n",
        "validation_dataset = image_dataset_from_directory(\n",
        "                        new_base_dir / 'validation',\n",
        "                        image_size=(180, 180),\n",
        "                        batch_size=32)\n",
        "\n",
        "test_dataset = image_dataset_from_directory(\n",
        "                new_base_dir / 'test',\n",
        "                image_size=(180, 180),\n",
        "                batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        },
        "id": "PIx8_4_TpVUE",
        "outputId": "66d55819-479f-4919-b392-6caee4466c40"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-f56b5fc14066>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimage_dataset_from_directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m train_dataset = image_dataset_from_directory(  ## 겉보기엔 데이터셋으로 보이지만 제너레이터라는 기계이다.\n\u001b[1;32m      3\u001b[0m     \u001b[0mnew_base_dir\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mimage_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m180\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m180\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     batch_size=32)\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'image_dataset_from_directory' from 'tensorflow.keras.utils' (/usr/local/lib/python3.8/dist-packages/tensorflow/keras/utils/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mj66EalPqXmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wZUDDI6usjov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8P7eYyzIsjsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN\n",
        "\n",
        "완전 연결 네트워크의 문제점으로부터 시작\n",
        "\n",
        "- 매개변수의 폭발적인 증가\n",
        "- 공간 추론의 부족\n",
        " - 픽셀 사이의 근접성 개념이 완전 연결 계층에서는 손실됨\n",
        "- 합성곱 계층은 입력 이미지가 커져도 튜닝해야 할 매개변수 개수에 영향을 주지 않음\n",
        "- 또한 그 어떠한 이미지에도 그 차원 수와 상관없이 적용될 수 있음\n",
        "\n",
        "CNN\n",
        "\n",
        "완전 연결 네트워크의 문제점으로부터 시작\n",
        "\n",
        "- 매개변수의 폭발적인 증가\n",
        "- 공간 추론의 부족\n",
        " - 픽셀 사이의 근접성 개념이 완전 연결 계층에서는 손실됨\n",
        "- 합성곱 계층은 입력 이미지가 커져도 튜닝해야 할 매개변수 개수에 영향을 주지 않음\n",
        "- 또한 그 어떠한 이미지에도 그 차원 수와 상관없이 적용될 수 있음\n",
        "\n",
        "https://medium.com/@pechyonkin/key-deep-learning-architectures-lenet-5-6fc3c59e6f4\n"
      ],
      "metadata": {
        "id": "3U9XJaZdtbTu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "컨볼루션 연산\n",
        "- 필터 연산\n",
        " - 입력 데이터에 필터를 통한 어떠한 연산을 진행\n",
        " - 필터에 대응하는 원소끼리 곱하고 그 합을 구함\n",
        " - 연산이 완료된 결과 데이터를 특징 맵이라 부름\n",
        "- 필터\n",
        " - 커널이라고도 하며 흔히 사진 어플에서 사용하는 '이미지 필터'와 비슷한 개념\n",
        " - 필터의 사이즈는 '거의 항상 홀수'\n",
        "  - 짝수이면 패딩이 비대칭이 되어버림\n",
        "  - 왼쪽, 오른쪽을 다르게 주어야함\n",
        "  - 중심위치가 존재, 즉 구별된 하나의 픽셀(중심 픽셀)이 존재\n",
        " - 필터의 학습 파라미터 개수는 입력 데이터의 크기와 상관없이 일정, 따라서 과적합을 방지할 수 있음\n",
        "\n",
        "An example of convolution operation  \n",
        "https://www.researchgate.net/figure/An-example-of-convolution-operation-in-2D-2_fig3_324165524\n",
        "\n",
        "- 일반적으로 합성곱 연산을 한 후의 데이터 사이즈는 (n - f + 1) x (n - f + 1)\n",
        "\n",
        "- 패딩과 스트라이드\n",
        " - 필터(커널) 사이즈와 함께 입력 이미지와 출력 이미지의 사이즈를 결정하기 위해 사용\n",
        " - 사용자가 결정할 수 있음\n",
        " - 패딩은 입력 데이터의 주변을 특정 값으로 채우는 기법. 주로 0으로 많이 채움\n",
        " - 출력 데이터의 크기 : (n + 2p - f +1) x (n + 2p -f + 1)\n",
        " - valid(패딩을 주지 않음) 와 same(패딩을 주어 입력 이미지와 연산 후의 이미지가 같게 함), 패딩의 크기는 (k-1)/2 (단, stride=1)\n",
        " - 스트라이드는 필터를 적용하는 간격을 의미\n",
        "\n",
        " https://kingnamji.tistory.com/24\n",
        "\n",
        " https://m.blog.naver.com/jevida/221841296542\n"
      ],
      "metadata": {
        "id": "U1NL1L_dtn9m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "풀링(Pooling)\n",
        "- 필터 사이즈 내에서 특정 값을 추출하는 과정\n",
        "- Max Pooling : 출력 데이터의 사이즈 계산은 컨볼루션 연산과 동일\n",
        "- 특징맵의 크기를 절반으로 줄임\n",
        "- 모델이 물체의 주요한 특성을 학습하도록 해주며 컨볼루션 신경망이 이동 불변성 특성을 가지게 해줌\n",
        "- 모델의 파라미터 개수를 줄여주고 연산 속도를 빠르게 해줌\n",
        "\n",
        "https://cs231n.github.io/convolutional-networks/\n"
      ],
      "metadata": {
        "id": "_p3GoMiavs0h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LeNet-5 \n",
        "- LeNet-5는 복잡하지 않은 망을 이용하여 (당시 기준)높은 성능을 보여주었을 뿐만 아니라 Convolutional layer와 pooling의 조합을 반복하는 현대적인 CNN 구조를 제안했다는 점에서 의미가 있는 모델 \n",
        "- https://velog.io/@woojinn8/CNN-Network-1.-LeNet\n"
      ],
      "metadata": {
        "id": "jow43WPbw0sX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visual Geometry Group Net(VGGNet)\n",
        "- 활성화 함수로 ReLU 사용, Dropout 적용\n",
        "- 합성곱과 풀링 계층으로 구성된 블록과 분류를 위한 완전 연결계층으로 결합된 전형적인 구조\n",
        "- 인위적으로 데이터셋을 늘림\n",
        " - 이미지 변환, 좌우 반전 등의 변환을 시도\n",
        "- 몇 개의 합성곱 계층과 최대 풀링 계층이 따르는 5개의 블록과 3개의 완전 연결계층으로 구성\n",
        "- 모든 합성곱과 최대 풀링 계층에 padding='SAME' 적용<br>\n",
        "https://buomsoo-kim.github.io/keras/2018/05/02/Easy-deep-learning-with-Keras-8.md/ <br>\n",
        "- 합성곱 계층에는 stride=1, 활성화 함수로 ReLU 사용\n",
        "- 특성맵 깊이를 증가 시킴\n",
        "- 척도 변경을 통한 데이터 보강(Data Augmentation)\n",
        "- 3x3 커널(커널은 필터를 말한다.)을 갖는 두 합성곱 계층을 쌓은 스택이 5x5 커널을 갖는 하나의 합성곱 계층과 동일한 수용영역(ERF)을 가짐\n",
        "- 11X11 사이즈의 필터 크기를 가지는 AlexNet과 비교하여 더 작은 합성곱 계층을 더 많이 포함해 더 큰 ERF를 얻음\n",
        "- 이와 같이 합성곱 계층의 개수가 많아지면 매개변수 개수를 줄이고 비선형성을 증가시킴\n",
        "- VGG-19 아키텍쳐는 VGG-16에 3개의 합성곱 계층을 추가\n",
        "\n",
        "- LeNet-5, AlexNet, VGG-16, ResNet, Inception Network  \n",
        "https://wooono.tistory.com/233\n"
      ],
      "metadata": {
        "id": "Jv2JC7BYxbwf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "드롭아웃은 과대적합을 방지하기 위해서 사용한다.\n",
        "\n",
        "어떻게 드롭아웃을 사용?\n",
        "\n",
        "드롭아웃의 정해준 비율에 따라서 그냥 없애버림"
      ],
      "metadata": {
        "id": "mk3OOXqBxnP3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- VGG-19 아키텍쳐\n",
        "\n",
        "  - VGG-16에 3개의 합성곱 계층을 추가\n",
        "\n",
        "  <br>   \n",
        "\n",
        "  <img src=\"https://neurohive.io/wp-content/uploads/2018/11/vgg16.png\">\n",
        "  <center>VGG-16 아키텍쳐</center>\n",
        "\n",
        "  <sub>[이미지 출처] https://neurohive.io/en/popular-networks/vgg16/ </sub>\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "- (참고) ILSVRC의 주요 분류 metric 중 하나는 `top-5`\n",
        "  \n",
        "  - 상위 5개 예측 안에 정확한 클래스가 포함되면 제대로 예측한 것으로 간주\n",
        "\n",
        "  - 일반적인 `top-k` metric의 특정 케이스\n"
      ],
      "metadata": {
        "id": "ewWlyGJ9zpRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import VGG16\n",
        "vgg_net = VGG16(include_top=True, weights='imagenet',\n",
        "                input_tensor=None, input_shape=None,\n",
        "                pooling=None, classes=1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfOwpPm4xcMv",
        "outputId": "180ed2e6-671c-46f5-dc4f-7bf05051eb8c"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
            "553467904/553467096 [==============================] - 4s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vgg_net.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "punix6zdzeMs",
        "outputId": "aefb1d9e-a561-4fba-caa2-220136d13200"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_6 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "fc1 (Dense)                  (None, 4096)              102764544 \n",
            "_________________________________________________________________\n",
            "fc2 (Dense)                  (None, 4096)              16781312  \n",
            "_________________________________________________________________\n",
            "predictions (Dense)          (None, 1000)              4097000   \n",
            "=================================================================\n",
            "Total params: 138,357,544\n",
            "Trainable params: 138,357,544\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- VGG-19 아키텍쳐\n",
        "\n",
        "  - VGG-16에 3개의 합성곱 계층을 추가\n",
        "\n",
        "  <br>   \n",
        "\n",
        "  <img src=\"https://neurohive.io/wp-content/uploads/2018/11/vgg16.png\">\n",
        "  <center>VGG-16 아키텍쳐</center>\n",
        "\n",
        "  <sub>[이미지 출처] https://neurohive.io/en/popular-networks/vgg16/ </sub>\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "- (참고) ILSVRC의 주요 분류 metric 중 하나는 `top-5`\n",
        "  \n",
        "  - 상위 5개 예측 안에 정확한 클래스가 포함되면 제대로 예측한 것으로 간주\n",
        "\n",
        "  - 일반적인 `top-k` metric의 특정 케이스\n"
      ],
      "metadata": {
        "id": "qoc94k9k0Nov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import VGG19\n",
        "vgg_net = VGG16(include_top=True, weights='imagenet',\n",
        "                input_tensor=None, input_shape=None,\n",
        "                pooling=None, classes=1000)"
      ],
      "metadata": {
        "id": "_FixdA800Shj"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vgg_net.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eD0ihjHc0hBX",
        "outputId": "d7151842-74dc-4c62-c3d7-55c855524c23"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_7 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "fc1 (Dense)                  (None, 4096)              102764544 \n",
            "_________________________________________________________________\n",
            "fc2 (Dense)                  (None, 4096)              16781312  \n",
            "_________________________________________________________________\n",
            "predictions (Dense)          (None, 1000)              4097000   \n",
            "=================================================================\n",
            "Total params: 138,357,544\n",
            "Trainable params: 138,357,544\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GoogLeNet, Inception 모듈\n",
        "- VGGNet을 제치고 같은 해 분류 과제에서 1등을 차지\n",
        "- 인셉션 블록이라는 개념을 도입하여 인셉션 네트워크라고도 불림\n",
        "- Inception Module은 layer에 1x1 Convolution layer를 추가해 bottleneck layer를 구현함으로써, channel 수를 감소시키며, 연산량을 줄이는 구조입니다.\n",
        "이것이 inception module의 기본 아이디어이며, Inception Network는 이러한 Inception Module의 집합입니다.\n"
      ],
      "metadata": {
        "id": "3d7yVnTd1o6n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GoogLeNet, Inception 모듈\n",
        "\n",
        "- VGGNet을 제치고 같은 해 분류 과제에서 1등을 차지\n",
        "\n",
        "- 인셉션 블록이라는 개념을 도입하여, **인셉션 네트워크(Inception Network)**라고도 불림\n",
        "\n",
        "\n",
        "  <img src=\"https://miro.medium.com/max/2800/0*rbWRzjKvoGt9W3Mf.png\">\n",
        "\n",
        "  <sub>[이미지 출처] https://medium.com/analytics-vidhya/cnns-architectures-lenet-alexnet-vgg-googlenet-resnet-and-more-666091488df5</sub>\n"
      ],
      "metadata": {
        "id": "vBb2Uplh2ba_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 특징 \n",
        "  \n",
        "  - CNN 계산 용량을 최적화하는 것을 고려\n",
        "\n",
        "  - 전형적인 합성곱, 풀링 계층으로 시작하고, 이 정보는 9개의 인셉션 모듈 스택을 통과  \n",
        "    해당 모듈을 하위 네트워크라고도 함\n",
        "\n",
        "  - 각 모듈에서 입력 특징 맵은 서로 다른 계층으로 구성된 4개의 병렬 하위 블록에 전달되고, 이를 서로 다시 연결\n",
        "\n",
        "  - 모든 합성곱과 풀링 계층의 padding옵션은 \"SAME\"이며 `stride=1`,  \n",
        "    활성화 함수는 `ReLU` 사용\n",
        "\n",
        "- 기여\n",
        "\n",
        "  - 규모가 큰 블록과 병목을 보편화\n",
        "\n",
        "  - 병목 계층으로 1x1 합성곱 계층 사용\n",
        "\n",
        "  - 완전 연결 계층 대신 풀링 계층 사용\n",
        "\n",
        "  - 중간 소실로 경사 소실 문제 해결\n",
        "\n",
        "  <img src=\"https://norman3.github.io/papers/images/google_inception/f01.png\">\n",
        "\n",
        "  <sub>[이미지 출처] https://norman3.github.io/papers/docs/google_inception.html</sub>\n"
      ],
      "metadata": {
        "id": "XLYMPEsa286o"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ow2McsNr0h59"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}