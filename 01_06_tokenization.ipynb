{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPJ+t1TgSr2e5CTEaUuYZ20",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zzwony/Start_0920/blob/main/01_06_tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vjMRXNwdqD0",
        "outputId": "0ffcb81d-068d-417c-83ed-1a406b6fa00b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ],
      "source": [
        "# gdrive에 연결\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 의존성 패키지 설치\n",
        "!pip install ratsnlp"
      ],
      "metadata": {
        "id": "U-_d-GKPd3qK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPT 입력값 만들기\n",
        "\n",
        "GPT 모델 입력값을 만들려면 Byte-level Byte Pair Encoding 어휘집합 구축 결과(`vocab.json`, `merges.txt`)가 자신의 구글 드라이브 경로(`/gdrive/My Drive/nlpbook/wordpiece`)에 있어야 합니다. 다음을 수행해 이미 만들어 놓은 BBPE 어휘집합을 포함한 GPT 토크나이저를 `tokenizer_gpt`라는 변수로 선언합니다.\n"
      ],
      "metadata": {
        "id": "xgtHlRf4etD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GPT 선언\n",
        "from transformers import GPT2Tokenizer\n",
        "tokenizer_gpt = GPT2Tokenizer.from_pretrained('/gdrive/MyDrive/nlpbook/bbpe')\n",
        "tokenizer_gpt.pad_token = '[PAD]'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2jrNQkSeAOW",
        "outputId": "ab0d1930-0290-49ff-b25e-969615182430"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "file /gdrive/MyDrive/nlpbook/bbpe/config.json not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "예시 문장 세 개를 각각 토큰화해보겠다."
      ],
      "metadata": {
        "id": "iUAJjEkHfMuV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\n",
        "    \"아 더빙.. 진짜 짜증나네요 목소리\",\n",
        "    \"흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나\",\n",
        "    \"별루 였다..\",\n",
        "]\n",
        "tokenized_sentences = [tokenizer_gpt.tokenize(sentence) for sentence in sentences]"
      ],
      "metadata": {
        "id": "CQppK9XQfYP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "토큰화 결과를 확인한다."
      ],
      "metadata": {
        "id": "nkBm8xshfn2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Fom3V4Efj4c",
        "outputId": "0463a029-284e-4afe-bee6-86bea541c3c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['ìķĦ', 'ĠëįĶë¹Ļ', '..', 'Ġì§Ħì§ľ', 'Ġì§ľì¦ĿëĤĺ', 'ëĦ¤ìļĶ', 'Ġëª©ìĨĮë¦¬'],\n",
              " ['íĿł',\n",
              "  '...',\n",
              "  'íı¬ìĬ¤íĦ°',\n",
              "  'ë³´ê³ł',\n",
              "  'Ġì´ĪëĶ©',\n",
              "  'ìĺģíĻĶ',\n",
              "  'ì¤Ħ',\n",
              "  '....',\n",
              "  'ìĺ¤ë²Ħ',\n",
              "  'ìĹ°ê¸°',\n",
              "  'ì¡°ì°¨',\n",
              "  'Ġê°Ģë³į',\n",
              "  'ì§Ģ',\n",
              "  'ĠìķĬ',\n",
              "  'êµ¬ëĤĺ'],\n",
              " ['ë³Ħë£¨', 'Ġìĺ', 'Ģëĭ¤', '..']]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이전 배치의 크기가 3이라고 가정하고 이번 배치의 입력값을 만들어 보겠다."
      ],
      "metadata": {
        "id": "JKtSbSu1fnJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\n",
        "    \"아 더빙.. 진짜 짜증나네요 목소리\",\n",
        "    \"흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나\",\n",
        "    \"별루 였다..\",\n",
        "]\n",
        "batch_inputs = tokenizer_gpt(\n",
        "    sentences,\n",
        "    padding='max_length',\n",
        "    max_length=12,\n",
        "    truncation=True,\n",
        ")"
      ],
      "metadata": {
        "id": "Of0QijIWf4LV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 코드의 실행결과로 1) input_ids 2) attention_mask가 생성되었다.\n",
        "\n",
        "batch_inputs의 내용을 확인해보겠다."
      ],
      "metadata": {
        "id": "92jEVuiQgYgl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_inputs.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eK7YLALFgg7P",
        "outputId": "195eed74-4eb9-420b-8167-4a391d21620d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['input_ids', 'attention_mask'])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_inputs['input_ids']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vb2o3jr5gics",
        "outputId": "4585c586-e52f-4996-a565-01fc728d74f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[334, 2338, 263, 581, 4055, 464, 3808, 0, 0, 0, 0, 0],\n",
              " [3693, 336, 2876, 758, 2883, 356, 806, 422, 9875, 875, 2960, 7292],\n",
              " [4957, 451, 3653, 263, 0, 0, 0, 0, 0, 0, 0, 0]]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "input_ids는 토큰화 결과를 가지고 각 토큰을 인덱스로 바꾼것이다. → 인덱싱 과정\n",
        "\n",
        "(vocab.json 기준)"
      ],
      "metadata": {
        "id": "0xxiU6L8g6Um"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_inputs['attention_mask']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijJjUICsgmy8",
        "outputId": "5e4fb29d-5ca3-490a-da36-9f15f8ee9404"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
              " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "attention_mask는 일반 토큰(1)과 패딩 토근(0)을 구분해 주는 장치이다."
      ],
      "metadata": {
        "id": "pFbhrowmgpSc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT"
      ],
      "metadata": {
        "id": "K0hmpupLhYHW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERT 입력값 만들기"
      ],
      "metadata": {
        "id": "dxoN-S2zoA3f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# BERT 토크나이저 선언\n",
        "from transformers import BertTokenizer\n",
        "tokenizer_bert = BertTokenizer.from_pretrained(\n",
        "    '/gdrive/MyDrive/nlpbook/wordpiece',\n",
        "    do_lower_case=False,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjiTWod2mSCN",
        "outputId": "b9f44fdc-0e63-4265-f8ac-ae7464f6c8b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "file /gdrive/MyDrive/nlpbook/wordpiece/config.json not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BERT 토크나이저로 토큰화하기\n",
        "sentences = [\n",
        "    \"아 더빙.. 진짜 짜증나네요 목소리\",\n",
        "    \"흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나\",\n",
        "    \"별루 였다..\",\n",
        "]\n",
        "tokenized_sentences = [tokenizer_bert.tokenize(sentence) for sentence in sentences]"
      ],
      "metadata": {
        "id": "-Q1zSKi-mmdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "예시 문장 3개를 워드피스 토크나이저로 토큰화한다.\n",
        "\n",
        "(##은 해당 토큰 어절의 시작이 아니라 앞선 토큰과 같은 위치에 있으며 연속됨을 표시함.)"
      ],
      "metadata": {
        "id": "TMg2sdmxoYQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4pkZ0qzm7yt",
        "outputId": "5c127b05-fd96-4ebb-f901-653a1bb0b301"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['아', '더빙', '.', '.', '진짜', '짜증나', '##네요', '목소리'],\n",
              " ['흠',\n",
              "  '.',\n",
              "  '.',\n",
              "  '.',\n",
              "  '포스터',\n",
              "  '##보고',\n",
              "  '초딩',\n",
              "  '##영화',\n",
              "  '##줄',\n",
              "  '.',\n",
              "  '.',\n",
              "  '.',\n",
              "  '.',\n",
              "  '오버',\n",
              "  '##연기',\n",
              "  '##조차',\n",
              "  '가볍',\n",
              "  '##지',\n",
              "  '않',\n",
              "  '##구나'],\n",
              " ['별루', '였다', '.', '.']]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BERT 모델 입력 만들기\n",
        "batch_inputs = tokenizer_bert(\n",
        "    sentences,\n",
        "    padding='max_length',\n",
        "    max_length=12,\n",
        "    truncation=True,\n",
        ")"
      ],
      "metadata": {
        "id": "yX2tpSUCm-X0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 코드의 실행 결과로 1) input_ids 2) attention_mask 3) token_type_ids 가 만들어진다."
      ],
      "metadata": {
        "id": "V_e0IrqrovU_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_inputs['input_ids']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdhMPFfinN8V",
        "outputId": "0dd1cfd8-abf6-461a-adf2-d1ce3e18267a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[2, 621, 2631, 16, 16, 1993, 3678, 1990, 3323, 3, 0, 0],\n",
              " [2, 997, 16, 16, 16, 2609, 2045, 2796, 1981, 1177, 16, 3],\n",
              " [2, 3274, 9508, 16, 16, 3, 0, 0, 0, 0, 0, 0]]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "출력물을 보면 맨 앞에는 2, 맨 뒤에는 3이 붙었다.\n",
        "\n",
        "이는 [CLS], [SEP] 라는 토큰에 대응하는 인덱스이며,\n",
        "\n",
        "BERT는 문장 시작과 끝에 이 2개의 토큰을 덧붙이는 특징이 있다."
      ],
      "metadata": {
        "id": "Rk2szCtfo7EO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_inputs['attention_mask']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axiAsQcbnZ18",
        "outputId": "9a819d9d-afc2-4dcf-e42c-584a1acec0d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
              " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT와 마찬가지로 일반 토큰이 자리한 곳은 1, 패딩 토큰이 자리한 곳은 0을 구분해준다."
      ],
      "metadata": {
        "id": "wqeIKI40pO_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_inputs['token_type_ids']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-Uyp5X-neVU",
        "outputId": "8c505e21-9e4f-4139-c4a4-7d5659cba339"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "문장이 하나만 들어가서 감성 분석을 할때는 0 이라는 신호를 주어서 하나의 문장만 들어가게 하고,\n",
        "\n",
        "0과 1이라는 신호를 주면 2개의 문장이 들어가서 추리&분석을 하라는 뜻이다.\n",
        "\n",
        "위의 출력 결과가 전부 0이 나온 이유는 이번 실습에서 문장을 하나씩 넣었기 때문이다."
      ],
      "metadata": {
        "id": "FAFngIOAnhc1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 언어 모델"
      ],
      "metadata": {
        "id": "osrFC2gB0cRt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 언어 모델: 단어 시퀀스에 확률을 부여하는 모델\n",
        "---\n",
        "- 결합 확률: '난폭' 이라는 단어와 '운전' 이라는 단어가 동시에 나타날 결합 확률을 말함.\n",
        " - P(무모, 운전) 보다는 P(난폭, 운전)이 큰 확률값을 지닌다.\n",
        "---\n",
        "- 조건부 확률: 결과가 되는 사건(운전)은 조건이 되는 사건(난폭)의 영향을 받아 변한다는 개념을 내포\n",
        " - 난폭이 나타난 다음에 운전이 나타날 확률\n",
        " - 언어 모델을 이전 단어들이 주어졌을 때 다음 단어가 나타날 확률을 부여하는 모델이라고 정의하기도 한다.\n",
        "---\n",
        "- 순방향 언어 모델: 문장 앞부터 뒤로, 사람이 이해하는 순서대로 계산하는 모델을 말함.\n",
        " - GPT, ELMo 같은 모델이 이런 방식으로 프리트레인을 수행함.\n",
        "---\n",
        "- 역방향 언어 모델: 문장 뒤부터 앞으로 계산하는 역방향 언어 모델\n",
        " - ELMo 같은 모델이 이런 방식으로 프리트레인을 수행함.\n",
        "---\n",
        "- P( w | context ) ; 넓은 의미의 언어 모델\n",
        " - 이는 컨텍스트(contect; 주변 맥락 정보)가 전제된 상태에서 특정 단어(w)가 나타날 조건부 확률을 나타냄.\n",
        " - 이렇게 정의된 언어 모델은 단어나 시퀀스로 구성된 컨텍스트를 입력받아 특정 단어가 나타날 확률을 출력함.\n",
        " - 이때 컨텍스트와 맞힐 단어를 어떻게 설정하는냐에 따라 다양하게 변형할 수 있다.\n",
        "   - 1) 마스크 언어 모델\n",
        "   - 2) 스킵-그램 모델\n",
        "---\n",
        "- 마스크 언어 모델\n",
        " - 학습 대상 문장에 빈칸을 만들어 놓고 해당 빈칸에 올 단어로 적절한 단어가 무엇일지 분류하는 과정으로 학습함.\n",
        " - BERT가 마스크 언어 모델로 프리트레인하는 대표적인 모델이다.\n",
        "   - [MASK] 카페 갔었어 거기 사람 많더라.\n",
        "   - 어제 [MASK] 갔었어 거기 사람 많더라.\n",
        "   - 어제 카페 [MASK] 거기 사람 많더라.\n",
        "   - 어제 카페 갔었어 [MASK] 사람 많더라.\n",
        "   - 어제 카페 갔었어 거기 [MASK] 많더라.\n",
        "   - 어제 카페 갔었어 거기 사람 [MASK].\n",
        "     - 여기서 [MASK] 부분은 맞혀야 할 타깃 단어를 가리킨다.\n",
        "\n",
        " - [장점] 맞힐 단어 이전 단어들만 참고할 수 있는 순방향, 역방향 언어 모델과 달리 마스크 언어 모델은 맞힐 단어를 계산할 때 문장 전체의 맥락을 참고함.\n",
        "  - 마스크 언어 모델에 양방향 성질이 있다고 함.\n",
        "---\n",
        "- 스캅-그램 모델\n",
        " - 어떤 단어 앞뒤에 특정 범위를 정해 두고 이 범위 내에 어떤 단어가 들어올지 분류한느 과정으로 학습함.\n",
        " - 컨텍스트로 설정한 단어(가려진 단어)의 앞뒤로 두 개씩 보는 상황이다.\n",
        "   - 어제 카페 [가려진 단어] 거기 사람\n",
        "   - 카페 갔었어 [가려진 단어] 사람 많더라\n",
        "     - 이때 스킵-그램 모델은 [갔었어] 주변에 어제, 카페, 거기, 사람이 나타날 확률을 각각 높이는 방식으로 학습함.\n",
        "     - 그 다음에 [거기] 주변에 카페, 갔었어, 사람, 많더라 가 나타날 확률을 각각 높인다.\n",
        "     - 즉, 스킵-그램 모델은 컨텍스트로 설정한 단어 주변에 어떤 단어들이 분포해 잇는지를 학습한다는 이야기.\n"
      ],
      "metadata": {
        "id": "kS_gk3Ug1Z1s"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ql0zKev82mPe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}