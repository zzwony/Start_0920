{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNtUz1g4UMySccPGQFBFRn+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zzwony/Start_0920/blob/main/01_09_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 셀프 어텐션 이해하기\n",
        "\n",
        "- 트랜스포머 구조에서 멀티 헤드 어텐션은 셀프 어텐션(self attention)이라고도 불립니다. \n",
        "\n",
        "- 트랜스포머 경쟁력의 원천은 셀프 어텐션에 있다.\n",
        "\n",
        "- 어텐션(attention)은 시퀀스 입력에 수행하는 기계학습 방법의 일종으로 시퀀스 요소들 가운데 태스크 수행에 중요한 요소에 집중하고 그렇지 않은 요소는 무시해 태스크 수행 성능을 끌어 올리며 기계 번역 과제에 처음 도입되었다.\n",
        "\n",
        "- 기계 번역에 어텐션을 도입한다면 타깃 언어를 디코딩할 때 소스 언어의 단어 시퀀스 가운데 디코딩에 도움되는 단어들 위주로 취사 선택해서 번역 품질을 끌어 올리게 된다. 즉, 어텐션은 디코딩할 때 소스 시퀀스 가운데 중요한 요소들만 추린다.\n",
        "\n",
        "- 셀프 어텐션이란, 말 그대로 자기 자신에 수행하는 어텐션 기법으로 입력 시퀀스 가운데 태스크 수행에 의미 있는 요소들 위주로 정보를 추출한다는 것이다.\n",
        "\n",
        "- 합성곱 신경망과 비교 : CNN은 합성곱 필터(convoltion filter) 라는 특수한 장치를 이용해 시퀀스의 지역적인 특징을 잡아내는 모델이다. 자연어는 기본적으로 시퀀스(단어 혹은 형태소의 나열)이고 특정 단어 기준 주변 문맥이 의미 형성에 중요한 역할을 하므로 CNN이 자연어 처리에 널리 쓰인다.하지만 CNN은 합성곱 필터 크기를 넘어서는 문맥은 읽어내기 어렵다는 단점이 있습니다. 예컨대 필터 크기가 3(3개 단어씩 처리)이라면 4칸 이상 떨어져 있는 단어 사이의 의미는 캐치하기 어렵다.\n",
        "\n",
        "- 순환 신경망과 비교 : RNN 역시 시퀀스 정보를 압축하는 데 강점이 있는 구조이다. 소스 언어 시퀀스인 어제, 카페, 갔었어, 거기, 사람, 많더라를 인코딩해야 한다고 가정해 보면 RNN은 소스 시퀀스를 차례대로 처리한다. 하지만 RNN은 시퀀스 길이가 길어질 수록 정보 압축에 문제가 발생하며 오래 전에 입력된 단어는 잊어버리거나, 특정 단어 정보를 과도하게 반영해 전체 정보를 왜곡하는 경우가 자주 생긴다.\n",
        "기계 번역을 할 때 RNN을 사용한다면 인코더가 디코더로 넘기는 정보는 소스 시퀀스의 마지막인 많더라라는 단어의 의미가 많이 반영될 수밖에 없으며 RNN은 입력 정보를 차례대로 처리하고 오래 전에 읽었던 단어는 잊어버리는 경향이 있다.\n",
        "\n",
        "- 어텐션과 비교 : cafe에 대응하는 소스 언어의 단어는 카페이고 이는 소스 시퀀스의 초반부에 등장한 상황에서 cafe라는 단어를 디코딩해야 할 때 카페를 반드시 참조해야 한다. 어텐션이 없는 단순 RNN을 사용하면 워낙 초반에 입력된 단어라 모델이 잊었을 가능성이 크고, 이 때문에 번역 품질이 낮아질 수 있다. 어텐션은 이러한 문제점을 해결하기 위해 제안되었으며 디코더 쪽 RNN에 어텐션을 추가하는 방식이다. 어텐션은 디코더가 타깃 시퀀스를 생성할 때 소스 시퀀스 전체에서 어떤 요소에 주목해야 할지 알려주므로 카페가 소스 시퀀스 초반에 등장하거나 소스 시퀀스의 길이가 길어지더라도 번역 품질이 떨어지는 것을 막을 수 있다.\n",
        "\n",
        "- 셀프 어텐션은 자기 자신에 수행하는 어텐션이다.입력 시퀀스가 어제, 카페, 갔었어, 거기, 사람, 많더라일 때 거기라는 단어가 어떤 의미를 가지는지 계산하는 상황에서 잘 학습된 셀프 어텐션 모델이라면 거기에 대응하는 장소는 카페라는 사실을 알아챌 수 있다. 그뿐만 아니라 거기는 갔었어와도 연관이 있음을 확인할 수 있다. 트랜스포머 인코더 블록 내부에서는 이처럼 거기라는 단어를 인코딩할 때 카페, 갔었어라는 단어의 의미를 강조해서 반영한다.(디코더에 넘겨줄 인코딩이다.)\n",
        "\n",
        "- 셀프 어텐션 수행 대상은 입력 시퀀스 전체이며 개별 단어와 전체 입력 시퀀스를 대상으로 어텐션 계산을 수행해 문맥 전체를 고려하기 때문에 지역적인 문맥만 보는 CNN 대비 강점이 있다. 아울러 모든 경우의 수를 고려(단어들 서로가 서로를 1대 1로 바라보게 함)하기 때문에 시퀀스 길이가 길어지더라도 정보를 잊거나 왜곡할 염려가 없다. 이는 RNN의 단점을 극복한 지점입이다.\n",
        "\n",
        "어텐션과 셀프 어텐션의 주요 차이\n",
        "- ★ 어텐션은 소스 시퀀스 전체 단어들(어제, 카페, …, 많더라)과 타깃 시퀀스 단어 하나(cafe) 사이를 연결하는 데 쓰인다. 반면 셀프 어텐션은 입력 시퀀스 전체 단어들(그림15, 그림16) 사이를 연결한다.\n",
        "- 어텐션은 RNN 구조 위에서 동작하지만 셀프 어텐션은 RNN 없이 동작한다.\n",
        "타깃 언어의 단어를 1개 생성할 때 어텐션은 1회 수행하지만 셀프어텐션은 인코더, 디코더 블록의 개수만큼 반복 수행한다.\n"
      ],
      "metadata": {
        "id": "p-FV_uJx3GsP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 일반화된 셀프 어텐션: 쿼리-키-값 모델\n",
        "\n",
        "- 셀프 어텐션은 쿼리(query), 키(key), 밸류(value) 세 가지 요소가 서로 영향을 주고 받는 구조이다. 트랜스포머 블록에는 문장 내 각 단어가 벡터(vector) 형태로 입력되는데 여기서 벡터란 숫자의 나열 정도로 이해할 수 있다.\n",
        "- 각 단어 벡터는 블록 내에서 어떤 계산 과정을 거쳐 쿼리, 키, 밸류 세 가지로 변환된다. 만일 트랜스포머 블록에 입력되는 문장이 여섯 개 단어로 구성돼 있다면 이 블록의 셀프 어텐션 계산 대상은 쿼리 벡터 6개, 키 벡터 6개, 밸류 백터 6개 등 모두 18개가 된다. 셀프 어텐션은 쿼리 단어 각각에 대해 모든 키 단어와 얼마나 유기적인 관계를 맺고 있는지 그 합이 1인 확률값으로 나타낸다. 카페라는 쿼리 단어와 가장 관련이 높은 키 단어는 거기라는 점(0.4)을 확인할 수 있다.\n",
        "- 셀프 어텐션 모듈은 밸류 벡터들을 가중합(weighted sum)하는 방식으로 계산을 마무리한다. 새롭게 만들어지는 카페 벡터( Z카페 )는 문장에 속한 모든 단어 쌍 사이의 관계가 녹아 있다.\n",
        "Z카페=0.1×V어제+0.1×V카페+0.1×V갔었어+0.4×V거기+0.2×V사람+0.1×V많더라\n",
        "- 카페에 대해서만 계산 예를 들었지만 이러한 방식으로 나머지 단어들도 셀프 어텐션을 각각 수행한다. 모드 시퀀스를 대상으로 셀프 어텐션 계산이 끝나면 그 결과를 다음 블록으로 넘깁니다. 이처럼 트랜스포머 모델은 셀프 어텐션을 블록(레이어) 수만큼 반복합니다.\n"
      ],
      "metadata": {
        "id": "h4bDHdEY596W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEi4Q5R09kOP",
        "outputId": "992053c3-6821-4a03-cf7f-339edb91f555"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "셀프 어텐션 출력 과정 : 쿼리, 키, 밸류 3개 요소 사이의 문맥적 관계성을 추출하는 과정\n",
        "- 입력(단어 개수, 단어 임베딩 차원 수)\n",
        "- 쿼리, 키, 밸류 만들기 : 세가지 행렬은 태스크를 가장 잘 수행하는 방향으로 학습 과정에서 업데이트\n",
        "- 셀프 어텐션 계산\n",
        "  - 쿼리 벡터들을 한꺼번에 모아서 키 벡터들과 행렬곱을 수행\n",
        "  - 소프트맥스 확률값 만들기 : 키 벡터의 차원수의 제곱근으로 나눠준 뒤 소프트맥스를 취하는 과정\n",
        "  - 소프트맥스 확률과 밸류를 가중합하기\n"
      ],
      "metadata": {
        "id": "1vD_dhAc6BQl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 목표: 셀프 어텐션 구하기\n",
        "# 입력 벡터 시퀀스로 쿼리, 키, 밸류 벡터들을 만드는 파트(단어 개수 3, 차원 수 4)\n",
        "\n",
        "# 변수 정의\n",
        "import torch\n",
        "\n",
        "# 입력 벡타 시퀀스(단어 개수3, 차원 수 4)\n",
        "x = torch.tensor([\n",
        "  [1.0, 0.0, 1.0, 0.0],\n",
        "  [0.0, 2.0, 0.0, 2.0],\n",
        "  [1.0, 1.0, 1.0, 1.0],  \n",
        "])\n",
        "\n",
        "# 셀프 어탠션은 쿼리, 키, 벨류 3개 요소 사이의 문맥적 관계성을 추출하는 과정\n",
        "# 쿼리 키, 밸류를 만들어 주는 행렬(w) → 가중치\n",
        "w_query = torch.tensor([\n",
        "  [1.0, 0.0, 1.0],\n",
        "  [1.0, 0.0, 0.0],\n",
        "  [0.0, 0.0, 1.0],\n",
        "  [0.0, 1.0, 1.0]\n",
        "])\n",
        "w_key = torch.tensor([\n",
        "  [0.0, 0.0, 1.0],\n",
        "  [1.0, 1.0, 0.0],\n",
        "  [0.0, 1.0, 0.0],\n",
        "  [1.0, 1.0, 0.0]\n",
        "])\n",
        "w_value = torch.tensor([\n",
        "  [0.0, 2.0, 0.0],\n",
        "  [0.0, 3.0, 0.0],\n",
        "  [1.0, 0.0, 3.0],\n",
        "  [1.0, 1.0, 0.0]\n",
        "])"
      ],
      "metadata": {
        "id": "vP4A-ftN94O7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "입력 벡타 시퀀스와 각각 정한 가중치의 행렬곱으로 계산하면 쿼리, 키, 밸류가 만들어진다."
      ],
      "metadata": {
        "id": "-mbdLhIf_3T7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 쿼리, 키, 밸류 만들기\n",
        "# w_key, w_query, w_value 세가지 행렬은 태스크를 가장 잘 수행할 수 있는 방향으로 학습 과정에서 업데이트\n",
        "# torch.matmul은 두 텐서의 행렬 곱을 출력한다.\n",
        "# [추가 설명] 두 텐서가 모두 1차원이면 내적(스칼라)이 반환되고, 두 인수가 모두 2차원이면 행렬-행렬 곱이 반환된다.\n",
        "\n",
        "keys = torch.matmul(x, w_key)\n",
        "querys = torch.matmul(x, w_query)\n",
        "values = torch.matmul(x, w_value)\n",
        "print(keys, '\\n')\n",
        "print(querys, '\\n')\n",
        "print(values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnLRlti0_ftu",
        "outputId": "f30ca1ca-9325-4374-9f2a-89f3c33f5bbe"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 1., 1.],\n",
            "        [4., 4., 0.],\n",
            "        [2., 3., 1.]]) \n",
            "\n",
            "tensor([[1., 0., 2.],\n",
            "        [2., 2., 2.],\n",
            "        [2., 1., 3.]]) \n",
            "\n",
            "tensor([[1., 2., 3.],\n",
            "        [2., 8., 0.],\n",
            "        [2., 6., 3.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 쿼리 벡터들을 한꺼번에 모아서 키 벡터들과 행렬곱을 수행\n",
        "attn_scores = torch.matmul(querys, keys.T)\n",
        "print(querys, '\\n')\n",
        "print(keys.T, '\\n')\n",
        "attn_scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zD1BosaHAk5X",
        "outputId": "426667a0-22b7-441a-9e6e-fa17b557cc49"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 2.],\n",
            "        [2., 2., 2.],\n",
            "        [2., 1., 3.]]) \n",
            "\n",
            "tensor([[0., 4., 2.],\n",
            "        [1., 4., 3.],\n",
            "        [1., 0., 1.]]) \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 2.,  4.,  4.],\n",
              "        [ 4., 16., 12.],\n",
              "        [ 4., 12., 10.]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "< 전치 시키는 이유 >\n",
        "\n",
        "첫 번재 행과 두 번째 열이 같아야 (차원이 같아야) 행렬곱을 할 수 있다.\n",
        "\n",
        "- 실습처럼 3x3으로 나오는 경우는 드물고 \n",
        "- 보통은 [1., 0., 2.] [1., 0., 2.] 이런식으로 나오기 때문에 잔차를 해주면\n",
        "- [1., 0., 2.]\n",
        "- [1., 0., 2.]\n",
        "\n",
        "이렇게 되서 행렬곱을 할 수 있다."
      ],
      "metadata": {
        "id": "DCs7sov5BmOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "print(keys, '\\n')\n",
        "print(keys.shape[-1], '\\n')   ## 뒤에 있는게 차원이니까 -1을 해주면 된다.\n",
        "np.sqrt(keys.shape[-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4rqGfWxD3qp",
        "outputId": "6541c589-f798-48bf-a9fa-5dff8aa52fe8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 1., 1.],\n",
            "        [4., 4., 0.],\n",
            "        [2., 3., 1.]]) \n",
            "\n",
            "3 \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.7320508075688772"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 소프트맥스 확률값 만들기: 키 벡터의 차원의 제곱근으로 나눠준 뒤 소프트맥스를 취하는 과정\n",
        "# 소프트 맥스에 넣으면 확률을 구할 수 있다.\n",
        "\n",
        "import numpy as np\n",
        "from torch.nn.functional import softmax\n",
        "\n",
        "key_dim_sqrt = np.sqrt(keys.shape[-1])\n",
        "attn_probs = softmax(attn_scores/key_dim_sqrt, dim=-1)\n",
        "attn_probs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRnyG0NDD_Jw",
        "outputId": "6f8b3047-5c73-400d-c7ec-a9b6cea5430f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.3613e-01, 4.3194e-01, 4.3194e-01],\n",
              "        [8.9045e-04, 9.0884e-01, 9.0267e-02],\n",
              "        [7.4449e-03, 7.5471e-01, 2.3785e-01]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 소프트맥스 확률과 밸류를 가중합하기\n",
        "\n",
        "weighted_values = torch.matmul(attn_probs, values)\n",
        "print(values, '\\n')\n",
        "weighted_values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hI2RQOzPFB0r",
        "outputId": "6e10b737-9582-47e4-bf59-2e52937a8a62"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 2., 3.],\n",
            "        [2., 8., 0.],\n",
            "        [2., 6., 3.]]) \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.8639, 6.3194, 1.7042],\n",
              "        [1.9991, 7.8141, 0.2735],\n",
              "        [1.9926, 7.4796, 0.7359]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습해서 계속 가중치를 업데이트 하는것이다.\n",
        "\n",
        "인풋된 문장들에 대한 의미를 잘 파악한 가중치를 계속 업데이트해서\n",
        "잘 압축된 상태로 디코더 쪽으로 넣어준다.\n",
        "\n",
        "인코더에서 작업할때는 의미를 잘 해석할 수 있게끔 출력하는게 셀프 어텐션에서 해준다.\n",
        "\n",
        "나온게 weighted_values이다."
      ],
      "metadata": {
        "id": "Gp9V_wGSFNay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 멀티 헤드 어텐션\n",
        "- 셀프 어텐션(self attention)을 여러 번 수행한 걸 가리킨다.\n",
        "- 여러 헤드가 독자적으로 셀프 어텐션을 계산한다는 이야기이다. 비유하자면 같은 문서(입력)를 두고 독자(헤드) 여러 명이 함꼐 읽는 구조\n",
        "- 멀티-헤드 어텐션은 개별 헤드의 셀프 어텐션 수행 결과를 이어붙이 행렬에 WO를 행렬 곱해서 마무리\n",
        "- 멀티-헤드 어텐션의 최종 수행 결과는 '입련 단어 수 x 목표 차원수'\n",
        "- 멀티 헤드 어텐션은 인코더, 디코더 블록 모두에 적용된다. 앞으로 특별한 언급이 없다면 셀프 어텐션은 멀티 헤드 어텐션인 것으로 이해"
      ],
      "metadata": {
        "id": "n8sKQuRqF22Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "트랜스포머 모델 학습 및 인퍼런스: 기계 번역을 수행(인퍼런스)\n",
        "- 소스 언어(한국어) 문장을 인코더에 입력해 인코더 마지막 블록의 단이 벡터 시퀀스를 추출\n",
        "- 인코더에서 넘어온 소스 언어 문장 정보와 디코더에 타깃 문장 시작을 알리는 스페셜 토큰 < s >를 넣어서, 타깃 언어(영어)의 첫 번째 토큰을 생성\n",
        "- 인코더 쪽에서 넘어온 소스 언어 문장 정보와 이전에 생성된 타깃 언어 토큰 시퀀스를 디코더에 넣어서 만든 정보로 타깃 언어의 다음 토근을 생성\n",
        "- 생성된 문장 길이가 충분하거나 문장 끝을 알리는 스페셜 토큰 < /s > 가 나올때까지 3을 반복"
      ],
      "metadata": {
        "id": "33rQgm6pOfXF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "트랜스포머에 적용된 기술\n",
        "\n",
        "- 인코더와 디코더 블록의 구조는 멀티 헤드 어텐션, 피드포워드 뉴럴 네트워크, 잔차 연결 및 레이어 정규화 등 세 가지 구성 요소를 기본\n",
        "\n",
        "- 피드포워드 뉴럴네트워크\n",
        "  - 입력은 현재 블록의 멀티 헤드 어텐션의의 개별 출력 벡터\n",
        "  - 피드포워드 뉴럴네트워크란 신경망(neural network)의 한 종류로 그림2와 같이 입력층(input layer,  x ), 은닉층(hidden layer,  h ), 출력층(ouput layer,  y ) 3개 계층으로 구성\n",
        "  - 이전 뉴런 값과 그에 해당하는 가중치를 가중합(weighted sum)한 결과에 바이어스(bias)를 더해 만듭니다. 가중치들과 바이어스는 학습 과정에서 업데이트. 활성 함수(activation function,  f )는 현재 계산하고 있는 뉴런의 출력을 일정 범위로 제한하는 역할. 활성함수는 ReLU(Rectified Linear Unit)\n",
        "  - 입력층 뉴런이 각각  [2,1] 이고 그에 해당하는 가중치가  [3,2] , 바이어스(bias)가 1이라고 가정해 보겠습니다. 그러면 은닉층 첫번째 뉴런 값은  2×3+1×2+1=9 가 되며 이 값은 양수이므로 ReLU를 통과해도 그대로 유지\n",
        "  - ★ 트랜스포머에서는 은닉층의 뉴런 갯수(즉 은닉층의 차원수)를 입력층의 네 배로 설정. 예컨대 피드포워드 뉴럴네트워크의 입력 벡터가 768차원일 경우 은닉층은 2048차원까지 늘렸다가 출력층에서 이를 다시 768차원으로 줄인다.\n",
        "\n",
        "- 잔차 연결\n",
        "  - 잔차 연결이란 블록(block) 계산을 건너뛰는 경로를 하나 두는 것을 의미\n",
        "  - 입력을  x , 이번 계산 대상 블록을  F 라고 할 때 잔차 연결은  F(x)+x 로 간단히 실현\n",
        "  - 잔차 연결을 두지 않았을 때는  f1 ,  f2 ,  f3 을 연속으로 수행(직렬 수행)하는 경로 한 가지만 존재하였으나, 잔차 연결을 블록마다 설정해둠으로써 모두 8가지의 새로운 경로가 생성. 다시 말해 모델이 다양한 관점에서 블록 계산을 수행\n",
        "  - 잔차 연결은 모델 중간에 블록을 건너뛰는 경로를 설정함으로써 학습을 용이하게 하는 효과가 있음 (다양한 관점에서 블록 계산을 수행하게 한다.)\n",
        "\n",
        "- 레이어 정규화\n",
        "  - 미니 배치의 인스턴스( x )별로 평균을 빼주고 표준편차로 나눠줘 정규화(normalization)을 수행하는 기법\n",
        "  - 레이어 정규화를 수행하면 학습이 안정되고 그 속도가 빨라지는 등의 효과\n",
        "  - 학습 과정은 미니 배치 단위로 이루어지며 이는 눈을 가린 상태에서 산등성이를 한걸음씩 내려가는 과정에 비유할 수 있다. 내가 지금 있는 위치에서 360도 모든 방향에 대해 한발한발 내딛어보고 가장 경사가 급한 쪽으로 한걸음씩 내려가는 과정을 반복하는 것이다.\n",
        "  - 모델을 업데이트할 때(산등성이를 내려갈 때) 중요한 것은 방향과 보폭인데 이는 최적화 도구(optimizer)의 도움을 받는다. 트랜스포머 모델이 쓰는 최적화 도구가 바로 아담 옵티마이저(Adam Optimizer)이며 아담 옵티마이저는 오차를 줄이는 성능이 좋아서 트랜스포머 말고도 널리 쓰인다.\n",
        "  - 아담 옵티마이저의 핵심 동작 원리는 방향과 보폭을 적절하게 정해주는 것이다. 방향을 정할 때는 현재 위치에서 가장 경사가 급한 쪽으로 내려가되, 여태까지 내려오던 관성(방향)을 일부 유지하도록 한다. 보폭의 경우 안가본 곳은 성큼 빠르게 걸어 훑고 많이 가본 곳은 갈수록 보폭을 줄여 세밀하게 탐색하는 방식으로 정한다.\n",
        "\n",
        "- 드롭아웃\n",
        "  - 딥러닝 모델은 그 표현력이 아주 좋아서 학습 데이터 그 자체를 외워버릴 염려가 있습니다. 이를 과적합(overfitting)이라고 합니다. 드롭아웃(dropout)은 이러한 과적합 현상을 방지하고자 뉴런의 일부를 확률적으로 0으로 대치하여 계산에서 제외하는 기법\n",
        "  - torch.nn.Dropout 객체는 뉴런별로 드롭아웃을 수행할지 말지를 확률적으로 결정하는 함수인데 p=0.2라는 말은 드롭아웃 수행 비율이 평균적으로 20%가 되게끔 하는 것임\n",
        "\n",
        "- 옵티마이저(딥러닝의 기본 원리이다.)\n",
        "  - 딥러닝 모델 학습은 모델 출력과 정답 사이의 오차(error)를 최소화하는 방향을 구하고 이 방향에 맞춰 모델 전체를 파라미처(parameter)들을 업데이트하는 과정. 이때 오차를 최소화하는 방향을 그래디언트(gradient)라고 하며 오차를 최소화하는 과정을 최적화(optimization)라고 함.\n",
        "  - 오차를 구하려면 현재 시점의 모델에 입력을 넣어봐서 처음부터 끝까지 계산해보고 정답과 비교해야 하며 오차를 구하기 위해 이같이 모델 처음부터 끝까지 순서대로 계산해보는 과정을 순전차(forward propagation)이라고 한다.\n",
        "  - 오차를 구했다면 오차를 최소화하는 최초의 그래디언트를 구할 수 있으며 이는 미분으로 구합니다. 이후 미분의 연쇄법칙(chain rule)에 따라 모델 각 가중치별 그래디언트 역시 구할 수 있다. 이 과정은 순전파의 역순으로 순차적으로 수행되는데 이를 역전파라고 함\n",
        "  - 학습 과정은 미니 배치 단위로 이루어지며 이는 눈을 가린 상태에서 산등성이를 한걸음씩 내려가는 과정에 비유할 수 있다. 내가 지금 있는 위치에서 360도 모든 방향에 대해 한발한발 내딛어보고 가장 경사가 급한 쪽으로 한걸음씩 내려가는 과정을 반복하는 것이다.\n",
        "  - 모델을 업데이트 할 때(산등성이를 내려갈 때) 중요한 것은 방향과 보폭인데 이는 최적화 도구(optimizer)의 도움을 받는다. 트랜스포머 모델이 쓰는 최적화 도구가 바로 아담 옵티마이저(Adam Optimizer)이며 아담 옵티마이저는 오차를 줄이는 성능이 좋아서 트랜스포머 말고도 널리 쓰인다.\n",
        "  - 아담 옵티마이저의 핵심 동작 원리는 방향과 보폭(보폭은 학습률을 말한다.)을 적절하게 정해주는 것이다. 방향을 정할 때는 현재 위치에서 가장 경사가 급한 쪽으로 내려가되, 여태까지 내려오던 관성(방향)을 일부 유지하도록 한다. 보폭의 경우 아가본 곳은 성큼 빠르게 걸어 훓고 많이 가본 곳은 갈수록 보폭을 줄여 세밀하게 탐색하는 방식으로 정한다.\n",
        "  - 관성을 유지한다는 모멘텀을 이용해 건너 뛰는걸 체크하며 우린 rms를 많이 사용했다."
      ],
      "metadata": {
        "id": "591gMR8_GnHA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "< 정리 >\n",
        "\n",
        "트렌스포머가 무엇인가?\n",
        "- 자연어를 변환해주는것을 말한다.\n",
        "\n",
        "자연어(텍스트 등)가 들어오면 인코더와 디코더를 이용해서 처리해준다.\n",
        "- 인코더에서는 들어오는 것에 대해서 잘 이해할 수 있도록 잘 압축\n",
        "- 디코더에서는 그걸 타깃의 목적에 맞게끔 잘 해준다. ex) 번역 등등\n",
        "\n",
        "트렌스포마가 각광 받게 된 이유는?\n",
        "- 셀프 어텐션을 사용하니 기존의 자연어 처리보다 성능이 좋아져서.\n",
        "\n",
        "셀프 어텐션이 왜 중요한가?\n",
        "- 셀프 어텐션은 자신을 중심으로 문맥을 전부 따져보며\n",
        "- 병렬적으로 수행하니 속도가 빠르다.\n",
        "- 딥려닝에서 배웠던 여러가지 요소들(전치, 피드 포워드 등등)이 들어간다.\n",
        "\n",
        "현존하는 자연어 처리 기법에서 가장 우수한 BERT, GPT를 기반으로 한다.\n",
        "- 인코더만 사용하는건 BERT이며, 디코더만 사용하는건 GPT이다.\n",
        "\n",
        "셀프 어텐션에서 이뤄지는 과정을 잘 알아야한다.\n",
        "- input data가 들어오면 가중치를 업데이트 하는건데\n",
        "- 딥러닝의 가장 핵심적인 것은 손실과 비용을 최소화하는 과정에서\n",
        "- 옵티마이저가 가이드 해주는 것에 따라서 최적으로 비용을 최소화하는 방향으로 업데이트 한다.\n",
        "- 360도에서 어느 곳이 경사가 급한가 그리고 그걸 찾아서 빠르게 산을 하산하는 과정과 같다."
      ],
      "metadata": {
        "id": "dY2bxMSeJttI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERT와 GPT 비교"
      ],
      "metadata": {
        "id": "0VNcgp_zNOEh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT와 GPT의 차이\n",
        "- GPT는 언어 모델, 일방향(문장 왼쪽부터 오른쪽으로 순차적으로 계산한다는 점)\n",
        "- BERT는 마스크 언어 모델, 양방향(빈칸 앞뒤 문맥을 모두 살필 수 있다는 점)\n",
        "- GPT는 문장 생성에, BERT는 문장의 의미를 추출하는데 강점을 지녔다.\n",
        "- GPT는 디코더만 취해서 사용, BERT는 인코더만 취해서 사용한다."
      ],
      "metadata": {
        "id": "JD8YXYK4k6Xs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DXyqHNoIlaYd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}